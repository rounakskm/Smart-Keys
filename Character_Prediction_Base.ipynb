{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Character_Prediction_Base.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"c58E6rYtySpT","colab_type":"text"},"cell_type":"markdown","source":["# **Character Prediction Engine**\n","\n","This is the heart of the smart-keys agent.\n","\n","Its a LSTM that will take in some representation of the input and return the next character in the sequence."]},{"metadata":{"id":"jdjNaK0UyAjY","colab_type":"code","outputId":"c45f5c82-7a15-4461-cb38-9b62009b4e6b","executionInfo":{"status":"ok","timestamp":1544589728467,"user_tz":300,"elapsed":12042,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"cell_type":"code","source":["# Code to check the resources available Ex. GPU RAM\n","# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting gputil\n","  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n","Building wheels for collected packages: gputil\n","  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.3.0\n","Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Gen RAM Free: 12.9 GB  | Proc size: 141.9 MB\n","GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"],"name":"stdout"}]},{"metadata":{"id":"9icAQ7ykdjZL","colab_type":"text"},"cell_type":"markdown","source":["#STEPS:\n","\n","Libraries & Dependencies:\n","1. Install PyTorch\n","2. Install any other dependencies/ libraries\n","3. Import all required libraries including PyTorch\n","\n","Getting, loading and preparing data:\n","4. Get text_corpus file from drive to colab instance\n","5. Read text_corpus file \n","6. Prepare the data from text_corpus file for training and testing \n","(Find set of characters in corpus, one-hot encode characters depending on vocabulary etc.)\n","\n","Create, Train and Validate the model:\n","7. Split the encoded text_corpus into train and validation data\n","8. Create the LSTM model\n","9. Train the data using the training encoded text_corpus\n","10. Validate the model using the validation text_corpus\n"]},{"metadata":{"id":"7chZmPb0yb2i","colab_type":"text"},"cell_type":"markdown","source":["### **Installing required libraries and dependencies**"]},{"metadata":{"id":"7vevyrEBx0Hr","colab_type":"code","outputId":"99310cdb-60bf-4413-deee-1e29f8d69f82","executionInfo":{"status":"ok","timestamp":1544701455691,"user_tz":300,"elapsed":37776,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["# Install pytorch\n","# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["tcmalloc: large alloc 1073750016 bytes == 0x5c32a000 @  0x7f110f0312a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"],"name":"stdout"}]},{"metadata":{"id":"bENuSPOGx4V-","colab_type":"code","outputId":"eb8f95ee-c32d-47b5-fc25-344fb6ad3af0","executionInfo":{"status":"ok","timestamp":1544701548166,"user_tz":300,"elapsed":1467,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"cell_type":"code","source":["#Check install and version\n","import torch\n","print(torch.__version__)\n","\n","# Check cuda avaialability\n","if torch.cuda.is_available():\n","   print(\"Yay!!\")\n","    "],"execution_count":2,"outputs":[{"output_type":"stream","text":["0.4.0\n","Yay!!\n"],"name":"stdout"}]},{"metadata":{"id":"QwI9iciwxxHj","colab_type":"code","outputId":"99a90b8a-ac5f-4ef8-ef2d-ebc6dc1604f6","executionInfo":{"status":"ok","timestamp":1544701578266,"user_tz":300,"elapsed":6983,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":319}},"cell_type":"code","source":["# Installing PyDrive\n","!pip install PyDrive"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\u001b[K    100% |████████████████████████████████| 993kB 20.1MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Building wheels for collected packages: PyDrive\n","  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n"],"name":"stdout"}]},{"metadata":{"id":"dUxKSNdHylMz","colab_type":"text"},"cell_type":"markdown","source":["### **Importing required libraries and dependencies**"]},{"metadata":{"id":"UEHnsBUizIwm","colab_type":"code","colab":{}},"cell_type":"code","source":["# NumPy & Pytorch\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"61VTbc3Hxr3x","colab_type":"code","colab":{}},"cell_type":"code","source":["# Set_trace is used for debugging\n","from IPython.core.debugger import set_trace"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AhUcwj8axvta","colab_type":"code","colab":{}},"cell_type":"code","source":["# PyDrive\n","# For importing data-set from Google Drive -> Colab VM \n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once in a notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1vQI9vgw01x_","colab_type":"text"},"cell_type":"markdown","source":["### **Loading text_corpus into the VM**"]},{"metadata":{"id":"pfR-qCi7d0iU","colab_type":"code","outputId":"2e48f7b9-004f-444a-d1a3-a098f1accb67","executionInfo":{"status":"ok","timestamp":1544701607043,"user_tz":300,"elapsed":2353,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["fileId = drive.CreateFile({'id': '1ZHd8wJx1o-O4O_mUxM8pfJKK7gX5SXZT'}) # id of the file being uploaded\n","print (fileId['title'])  # text_corpus\n","file = fileId['title']\n","fileId.GetContentFile(file)  # Save Drive file on Colab VM as a local file"],"execution_count":9,"outputs":[{"output_type":"stream","text":["text_corpus\n"],"name":"stdout"}]},{"metadata":{"id":"r7rAF12i0go8","colab_type":"code","outputId":"ba05f185-b246-4032-ab43-80fd00e7dcdf","executionInfo":{"status":"ok","timestamp":1544701611144,"user_tz":300,"elapsed":3503,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Check if text_corlus is present in the current working directory\n","!ls"],"execution_count":10,"outputs":[{"output_type":"stream","text":["adc.json  sample_data  text_corpus\n"],"name":"stdout"}]},{"metadata":{"id":"adjRPkh91C7c","colab_type":"text"},"cell_type":"markdown","source":["### **Read text_corpus and prepare for training**"]},{"metadata":{"id":"QnbXidFf1K5g","colab_type":"code","colab":{}},"cell_type":"code","source":["with open('text_corpus', 'r') as f:\n","    text_corpus = f.read()\n","\n","# print(len(text_corpus))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4yUeTUaW1pf5","colab_type":"code","outputId":"b14f3e11-244e-4918-ba26-d1ff5e555368","executionInfo":{"status":"ok","timestamp":1544701615339,"user_tz":300,"elapsed":672,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Set of all unique characters in the corpus, will give us size of vocabulary \n","# Will have a-z, A-Z, special characters and numbers present in the text_corpus\n","characters = set(text_corpus)\n","\n","print(len(characters))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["103\n"],"name":"stdout"}]},{"metadata":{"id":"U1-gvn1IxBxe","colab_type":"code","outputId":"0765f7f3-9acb-40b5-c40b-67066f5f13eb","executionInfo":{"status":"ok","timestamp":1544701617597,"user_tz":300,"elapsed":653,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["# Creating a dictionary with integers as keys and charactes as values\n","integer_to_character = dict(enumerate(characters))\n","\n","print(integer_to_character)\n","\n","# Swapping the keys with the values to create a character to integer mapping\n","# Characters as keys and integers as values \n","character_to_integer = dict()\n","for index, char in integer_to_character.items():\n","  character_to_integer[char] = index\n","\n","print(len(character_to_integer))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["{0: 'œ', 1: 'R', 2: '3', 3: 'x', 4: 'o', 5: 'è', 6: 'F', 7: '[', 8: 'm', 9: '8', 10: 'h', 11: ' ', 12: 'ë', 13: '\\n', 14: 'B', 15: 'æ', 16: 'Q', 17: '‘', 18: 'c', 19: 'e', 20: 'k', 21: ';', 22: '1', 23: 'à', 24: 'G', 25: '£', 26: '!', 27: 'p', 28: 'P', 29: 'â', 30: '\"', 31: 'N', 32: 'W', 33: 'O', 34: '6', 35: '}', 36: ',', 37: 'L', 38: 'u', 39: '“', 40: 't', 41: '’', 42: '$', 43: '-', 44: 'S', 45: 'I', 46: 'g', 47: 'v', 48: '*', 49: 'w', 50: 'é', 51: ')', 52: 'i', 53: 'ö', 54: 'J', 55: 'Z', 56: 'r', 57: 'l', 58: 'd', 59: 'M', 60: '2', 61: '7', 62: '(', 63: '.', 64: 'a', 65: 'f', 66: '9', 67: 'ï', 68: ']', 69: 'y', 70: 'Y', 71: 's', 72: 'A', 73: '5', 74: 'X', 75: 'ê', 76: '0', 77: 'b', 78: 'E', 79: 'U', 80: 'q', 81: 'n', 82: '”', 83: '&', 84: '`', 85: '?', 86: 'j', 87: ':', 88: 'á', 89: 'V', 90: 'D', 91: 'ô', 92: 'T', 93: 'K', 94: '{', 95: 'H', 96: \"'\", 97: '/', 98: '4', 99: 'z', 100: '_', 101: '—', 102: 'C'}\n","103\n"],"name":"stdout"}]},{"metadata":{"id":"ujEL5JvayaqY","colab_type":"code","colab":{}},"cell_type":"code","source":["# Encoding the entire text_corpus using the character to integer mapping\n","\n","text_corpus_encoded = []\n","\n","for char in text_corpus:\n","  text_corpus_encoded.append(character_to_integer[char])\n","  \n","text_corpus_encoded = np.array(text_corpus_encoded)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_jds7iWF-hN0","colab_type":"text"},"cell_type":"markdown","source":["### **Custom Data-batches for training & testing**\n","The goal is to generate batches with features : stream of characters and targets : same stream of characters but shifted by one character sequentially \n","\n","The input to the data loader is the text_corpus but in encoded form (using character to integer mapping)"]},{"metadata":{"id":"QQ6IifYH2YhT","colab_type":"code","colab":{}},"cell_type":"code","source":["class TextDataLoader:\n","  def __init__(self, text_corpus_encoded, no_sequences_per_batch, no_of_characters):\n","    \n","    \n","    self.no_sequences = no_sequences_per_batch\n","    self.no_characters = no_of_characters\n","    \n","    self.batch_size = self.no_sequences * self.no_characters\n","    self.no_of_batches = len(text_corpus_encoded)//self.batch_size\n","    \n","    #print(f\"No. of Batches generated from given corpus = {self.no_of_batches}\")\n","    \n","    # remove extra characters, so we get full batches\n","    self.text_corpus_encoded = text_corpus_encoded[:self.batch_size * self.no_of_batches]\n","    \n","    # Arrange data in no_sequence_per_batch rows and rest columns\n","    self.text_corpus_encoded = self.text_corpus_encoded.reshape((self.no_sequences,-1))\n","    \n","  def make_batches(self):\n","    # Each step is number of characters apart\n","    for chars in range(0, self.text_corpus_encoded.shape[1], self.no_characters):\n","      \n","      features = self.text_corpus_encoded[:, chars:chars+self.no_characters]\n","      # Array of zeros with shape similar to features\n","      lables = np.zeros_like(features) \n","      \n","      try:\n","        lables[:, :-1] = features[:, 1:] \n","        lables[:, -1] = self.text_corpus_encoded[:, chars+self.no_characters]\n","      except IndexError:\n","        #print(\"in except\")\n","        lables[:, :-1] = features[:, 1:] \n","        lables[:, -1] = self.text_corpus_encoded[:, 0]\n","      \n","      yield features, lables\n","      \n","    \n","  \n","  def __len__(self):\n","    '''Return size of the data set'''\n","    return len(self.text_corpus_encoded)\n","  \n","  def __getitem__(self, idx):\n","    pass\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"CABSY8C7AQZZ","colab_type":"code","outputId":"5c5b598e-9b86-485f-aaf8-186dc05c6786","executionInfo":{"status":"ok","timestamp":1544701627934,"user_tz":300,"elapsed":656,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"cell_type":"code","source":["# Testing dataLoader and the generator\n","T = TextDataLoader(text_corpus_encoded, 128,128)\n","x,y = next(T.make_batches())\n","print(x[0])\n","print(\"*******\")\n","print(y[0])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[92 10 19 11 31 19 57 57 52 19 36 11 64 11 18 56 38 52 71 52 81 46 11 69\n"," 64 49 57 36 11 71 49 38 81 46 11 40  4 11 10 19 56 11 64 81 18 10  4 56\n"," 11 49 52 40 10  4 38 40 11 64 11 65 57 38 40 40 19 56 11  4 65 13 40 10\n"," 19 11 71 64 52 57 71 36 11 64 81 58 11 49 64 71 11 64 40 11 56 19 71 40\n"," 63 11 92 10 19 11 65 57  4  4 58 11 10 64 58 11  8 64 58 19 36 11 40 10\n"," 19 11 49 52 81 58 11 49]\n","*******\n","[10 19 11 31 19 57 57 52 19 36 11 64 11 18 56 38 52 71 52 81 46 11 69 64\n"," 49 57 36 11 71 49 38 81 46 11 40  4 11 10 19 56 11 64 81 18 10  4 56 11\n"," 49 52 40 10  4 38 40 11 64 11 65 57 38 40 40 19 56 11  4 65 13 40 10 19\n"," 11 71 64 52 57 71 36 11 64 81 58 11 49 64 71 11 64 40 11 56 19 71 40 63\n"," 11 92 10 19 11 65 57  4  4 58 11 10 64 58 11  8 64 58 19 36 11 40 10 19\n"," 11 49 52 81 58 11 49 64]\n"],"name":"stdout"}]},{"metadata":{"id":"pvxJ0VReEfUo","colab_type":"text"},"cell_type":"markdown","source":["### **The LongShortTermMemory Network **"]},{"metadata":{"id":"eJlcT7tYEx7X","colab_type":"code","colab":{}},"cell_type":"code","source":["# LSTM net with 4 LSTM layers, 2 Fully Connected layer and 2 dropout layers\n","# Each LSTM layer is a LSTM cell that takes hidden state and cell state as input \n","# and produces new hidden state and cell state as output\n","\n","# input_size = Vocabulary size\n","# Output of the network is also of Vocabulary size\n","\n","class LSTM(nn.ModuleList): \n","  def __init__(self, sequence_length, input_size, hidden_size, batch_size):\n","    super(LSTM,self).__init__()\n","    \n","    # used for live debugging, use next command to move to next line\n","    # set_trace()\n","    \n","    self.sequence_length = sequence_length\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.batch_size = batch_size\n","    \n","    # LSTMCell 1\n","    self.lstm1 = nn.LSTMCell(input_size = self.input_size, \n","                             hidden_size = self.hidden_size, \n","                             bias = True)\n","    \n","    # LSTMCell 2\n","    self.lstm2 = nn.LSTMCell(input_size = self.hidden_size, \n","                             hidden_size = self.hidden_size, \n","                             bias = True)\n","    \n","    # LSTMCell 3\n","    self.lstm3 = nn.LSTMCell(input_size = self.hidden_size , \n","                             hidden_size = self.hidden_size, \n","                             bias = True)\n","    \n","    # LSTMCell 4\n","    self.lstm4 = nn.LSTMCell(input_size = self.hidden_size, \n","                             hidden_size = self.hidden_size, \n","                             bias = True)\n","    \n","    # LSTMCell 5\n","    self.lstm5 = nn.LSTMCell(input_size = self.hidden_size, \n","                             hidden_size = self.hidden_size, \n","                             bias = True)\n","    \n","    # Dropout Layer 1\n","    self.drop1 = nn.Dropout(p=0.5)\n","    \n","    # Fully-connected Layer, out_features = input_size = vocab_size\n","    self.fc1 = nn.Linear(in_features = self.hidden_size,\n","                        out_features = self.hidden_size,\n","                        bias = True)\n","\n","    # Dropout Layer 2\n","    self.drop2 = nn.Dropout(p=0.5)\n","    \n","    self.fc2 = nn.Linear(in_features = self.hidden_size,\n","                        out_features = self.input_size,\n","                        bias = True)\n","    \n","    \n","  def forward(self,inputs, initial_states):\n","    '''\n","    Function responsible for the forward pass through the network\n","    \n","    Args:\n","      inputs         : Tuple of batch, input_size at each timestep t\n","      initail_states : Tuple of hidden state and cell state, initailly set to 0s\n","    \n","    Returns:\n","      Collection of output sequences at each timestep t\n","    '''\n","    \n","    # More debugguing\n","    # set_trace()\n","\n","    # Initialize empty output sequence\n","    output_sequence = torch.empty((self.sequence_length,\n","                         self.batch_size,\n","                         self.input_size))\n","    \n","    \n","    # For every time step in the sequence\n","    for t in range(self.sequence_length):\n","     \n","      # Passing the input sequentially through all LSTMCells\n","      initial_hidden_state, initial_cell_state = initial_states\n","      \n","      initial_hidden_state.requires_grad_()\n","      initial_hidden_state.cuda()\n","      \n","      initial_cell_state.requires_grad_()\n","      initial_cell_state.cuda()\n","      \n","      state1 = self.lstm1(inputs[t], (initial_hidden_state, initial_cell_state))\n","      hidden_state1, cell_state1 = state1\n","      \n","      state2 = self.lstm2(hidden_state1, (initial_hidden_state, initial_cell_state))\n","      hidden_state2, cell_state2 = state2\n","      \n","      state3 = self.lstm3(hidden_state2, (initial_hidden_state, initial_cell_state))\n","      hidden_state3, cell_state3 = state3\n","      \n","      state4 = self.lstm4(hidden_state3, (initial_hidden_state, initial_cell_state))\n","      hidden_state4, cell_state4 = state4\n","      \n","      state5 = self.lstm5(hidden_state3, (initial_hidden_state, initial_cell_state))\n","      hidden_state5, cell_state5 = state5\n","      \n","      # Passing output of LSTMCells through dropout and fc layers\n","      output = self.fc1(self.drop1(hidden_state5))\n","      \n","      # add final output to output sequence\n","      #output_sequence[t] = self.fc2(self.drop2(output))\n","      output_sequence[t] = self.fc2(output)\n","      \n","    # returning output sequence\n","    return output_sequence.view((self.sequence_length * self.batch_size, -1))\n","  \n","  def initialize_hidden_and_cell_states(self):\n","    # The hidden and cell state at the start are all zeros\n","    return (torch.zeros(self.batch_size, self.hidden_size, device=device),\n","            torch.zeros(self.batch_size, self.hidden_size, device=device))\n","\n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"8IDEsStChcB6","colab_type":"code","outputId":"55872819-a3cb-4499-c2af-6d585cf8d282","executionInfo":{"status":"ok","timestamp":1544701640015,"user_tz":300,"elapsed":5464,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Setting the device that will be used for training.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# Creating the model and load it into the GPU\n","model = LSTM(sequence_length=128, \n","             input_size=len(character_to_integer), \n","             hidden_size=512, \n","             batch_size=128)\n","model = model.to(device)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"metadata":{"id":"_Qrj1sySNOcS","colab_type":"code","colab":{}},"cell_type":"code","source":["# One-Hot encoding a tensor\n","def to_categorical(input_instance, num_classes):\n","  \"\"\"\n","  Function responsible for one-hot encoding a given tensor\n","  \n","  Args:\n","    input_instance : np.ndarray which is one instance of input that is to \n","                     be one-hot encoded\n","    num_classes    : total no. of classes\n","  \"\"\"\n","  return np.eye(num_classes, dtype='uint8')[input_instance]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"znWpPNOgRgEW","colab_type":"text"},"cell_type":"markdown","source":["### **Optimizer & Loss function**"]},{"metadata":{"id":"WDL3zUclRezu","colab_type":"code","colab":{}},"cell_type":"code","source":["# Using Adam as Optimizer and CrossEntropyLoss as the loss function\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 0.01)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O1l6nRuYSvDd","colab_type":"text"},"cell_type":"markdown","source":["### **Training & Validating**"]},{"metadata":{"id":"MQz9b0-VS1sJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Separating validation and training set from the encoded text corpus\n","# First 80% data used for training and last 20% data used for validation\n","\n","validation_index = int(len(text_corpus_encoded) *(1-0.2))\n","\n","validation_set = text_corpus_encoded[validation_index:]\n","training_set = text_corpus_encoded[:validation_index]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zzZtTgxkUGbD","colab_type":"code","outputId":"bf40eabb-853b-4169-fa46-994ca1ef83d6","executionInfo":{"status":"ok","timestamp":1544702267737,"user_tz":300,"elapsed":333237,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":9391}},"cell_type":"code","source":["# Training the model\n","for epoch in range(10):\n","  \n","  # Initialize hidden and cell state to zero at the beginning\n","  hidden_and_cell_state = model.initialize_hidden_and_cell_states()\n","  \n","  \n","  # Creating instance of dataloader class\n","  T = TextDataLoader(training_set, 128,128)\n","  \n","  # To find average loss per epoch\n","  batch_count = 0\n","  loss_sum = 0\n","  \n","  for i, (feature, label) in enumerate(T.make_batches()):\n","    \n","    # One-hot encode inputs, convert into tensor and transpose them\n","    feature = torch.from_numpy(to_categorical(feature, num_classes=model.input_size).transpose([1, 0, 2]))\n","    # Converting input to FloatTensor\n","    feature = feature.float()\n","    \n","    # Transpose labels and convert into LongTensor\n","    label = torch.from_numpy(label.T).type(torch.LongTensor)\n","    \n","    # Wrap features and labels in PyTorch Variable\n","    feature, label = Variable(feature),Variable(label)\n","    \n","    # Convert features and labels to type torch.cuda and load them into the GPU\n","    feature.requires_grad_()\n","    feature = feature.to(device)\n","   \n","    #label.requires_grad_()\n","    #label = feature.to(device)\n","    \n","    optimizer.zero_grad() # Makes sure the gradients are initially zero\n","    \n","    out = model(feature, hidden_and_cell_state) # Forward pass\n","    \n","    loss = criterion(out,label.contiguous().view(128*128)) # Computing loss\n","    \n","    loss.backward() # Back-Prop the loss / Backward Pass\n","\n","    optimizer.step() # Update the gradients\n","    \n","    print(f'Batch : {i+1} Loss : {loss}')\n","    \n","    loss_sum +=loss\n","    batch_count +=1\n","\n","  print(f'Epoch : {epoch+1} Average Loss : {loss_sum/batch_count}')\n","\n","    \n","    \n","    \n","    \n","  "],"execution_count":22,"outputs":[{"output_type":"stream","text":["Batch : 1 Loss : 4.6424713134765625\n","Batch : 2 Loss : 4.361251354217529\n","Batch : 3 Loss : 3.287141799926758\n","Batch : 4 Loss : 6.865812301635742\n","Batch : 5 Loss : 3.685643196105957\n","Batch : 6 Loss : 3.3834519386291504\n","Batch : 7 Loss : 3.4586434364318848\n","Batch : 8 Loss : 3.4660463333129883\n","Batch : 9 Loss : 3.4003405570983887\n","Batch : 10 Loss : 3.293982744216919\n","Batch : 11 Loss : 3.1509416103363037\n","Batch : 12 Loss : 3.1774771213531494\n","Batch : 13 Loss : 3.185244083404541\n","Batch : 14 Loss : 3.214449882507324\n","Batch : 15 Loss : 3.3372490406036377\n","Batch : 16 Loss : 3.1381304264068604\n","Batch : 17 Loss : 3.1278157234191895\n","Batch : 18 Loss : 3.0291531085968018\n","Batch : 19 Loss : 3.148047924041748\n","Batch : 20 Loss : 3.209872245788574\n","Batch : 21 Loss : 5.432459354400635\n","Batch : 22 Loss : 3.4241104125976562\n","Batch : 23 Loss : 3.632106065750122\n","Batch : 24 Loss : 3.176801919937134\n","Batch : 25 Loss : 3.3661158084869385\n","Batch : 26 Loss : 3.221060276031494\n","Batch : 27 Loss : 3.3445496559143066\n","Batch : 28 Loss : 3.162640333175659\n","Batch : 29 Loss : 3.0816450119018555\n","Batch : 30 Loss : 3.130828857421875\n","Batch : 31 Loss : 3.0539655685424805\n","Batch : 32 Loss : 3.0291056632995605\n","Batch : 33 Loss : 3.011833429336548\n","Batch : 34 Loss : 3.007467746734619\n","Batch : 35 Loss : 3.0286221504211426\n","Batch : 36 Loss : 3.0005199909210205\n","Batch : 37 Loss : 2.926626205444336\n","Batch : 38 Loss : 2.9892070293426514\n","Batch : 39 Loss : 2.922851085662842\n","Batch : 40 Loss : 2.943666696548462\n","Batch : 41 Loss : 2.9342360496520996\n","Batch : 42 Loss : 2.9378535747528076\n","Batch : 43 Loss : 2.9393835067749023\n","Batch : 44 Loss : 2.9281129837036133\n","Batch : 45 Loss : 2.8929576873779297\n","Batch : 46 Loss : 2.913041114807129\n","Batch : 47 Loss : 2.8761990070343018\n","Batch : 48 Loss : 2.889362096786499\n","Batch : 49 Loss : 2.8888137340545654\n","Batch : 50 Loss : 2.862039566040039\n","Batch : 51 Loss : 2.868713617324829\n","Batch : 52 Loss : 2.858515739440918\n","Batch : 53 Loss : 2.8452045917510986\n","Batch : 54 Loss : 2.8219406604766846\n","Batch : 55 Loss : 2.8533756732940674\n","Batch : 56 Loss : 2.8166580200195312\n","Batch : 57 Loss : 2.8506205081939697\n","Batch : 58 Loss : 2.8339481353759766\n","Batch : 59 Loss : 2.8385117053985596\n","Batch : 60 Loss : 2.829672336578369\n","Batch : 61 Loss : 2.824310064315796\n","Batch : 62 Loss : 2.8514745235443115\n","Batch : 63 Loss : 2.8106284141540527\n","Batch : 64 Loss : 2.8269872665405273\n","Batch : 65 Loss : 2.825016975402832\n","Batch : 66 Loss : 2.810079574584961\n","Batch : 67 Loss : 2.815910816192627\n","Batch : 68 Loss : 2.7742772102355957\n","Batch : 69 Loss : 2.770998001098633\n","Batch : 70 Loss : 2.7880523204803467\n","Batch : 71 Loss : 2.787642002105713\n","Batch : 72 Loss : 2.771817922592163\n","Batch : 73 Loss : 2.7872800827026367\n","Batch : 74 Loss : 2.766359329223633\n","Batch : 75 Loss : 2.742621898651123\n","Batch : 76 Loss : 2.7510647773742676\n","Batch : 77 Loss : 2.7504818439483643\n","Batch : 78 Loss : 2.752448081970215\n","Batch : 79 Loss : 2.7695116996765137\n","Batch : 80 Loss : 2.7569334506988525\n","Batch : 81 Loss : 2.7948827743530273\n","Batch : 82 Loss : 2.7367823123931885\n","Batch : 83 Loss : 2.746344566345215\n","Batch : 84 Loss : 2.745683193206787\n","Batch : 85 Loss : 2.7315220832824707\n","Batch : 86 Loss : 2.739694118499756\n","Batch : 87 Loss : 2.731508493423462\n","Batch : 88 Loss : 2.750554084777832\n","Batch : 89 Loss : 2.767974615097046\n","Batch : 90 Loss : 2.7266173362731934\n","Batch : 91 Loss : 2.7134900093078613\n","Batch : 92 Loss : 2.699401617050171\n","Batch : 93 Loss : 2.7258780002593994\n","Batch : 94 Loss : 2.727459192276001\n","Batch : 95 Loss : 2.7040867805480957\n","Batch : 96 Loss : 2.730583429336548\n","Batch : 97 Loss : 2.737090826034546\n","Batch : 98 Loss : 2.7089216709136963\n","Batch : 99 Loss : 2.702080488204956\n","Batch : 100 Loss : 2.726713180541992\n","Batch : 101 Loss : 2.73880672454834\n","Batch : 102 Loss : 2.6922171115875244\n","Batch : 103 Loss : 2.6926040649414062\n","Batch : 104 Loss : 2.7007570266723633\n","Batch : 105 Loss : 2.7073590755462646\n","Batch : 106 Loss : 2.698174476623535\n","Batch : 107 Loss : 2.690542697906494\n","Batch : 108 Loss : 2.696497678756714\n","Batch : 109 Loss : 2.7060935497283936\n","Batch : 110 Loss : 2.682455062866211\n","Batch : 111 Loss : 2.6999716758728027\n","Batch : 112 Loss : 2.6720807552337646\n","Batch : 113 Loss : 2.673555374145508\n","Batch : 114 Loss : 2.68910813331604\n","Batch : 115 Loss : 2.6864686012268066\n","Batch : 116 Loss : 2.666785955429077\n","Batch : 117 Loss : 2.697431802749634\n","Batch : 118 Loss : 2.693469524383545\n","Batch : 119 Loss : 2.6833345890045166\n","Batch : 120 Loss : 2.68254017829895\n","Batch : 121 Loss : 2.6699986457824707\n","Batch : 122 Loss : 2.6591193675994873\n","Batch : 123 Loss : 2.6666946411132812\n","Batch : 124 Loss : 2.685652017593384\n","Batch : 125 Loss : 2.710102081298828\n","Batch : 126 Loss : 2.675335645675659\n","Batch : 127 Loss : 2.683849573135376\n","Batch : 128 Loss : 2.657505989074707\n","Batch : 129 Loss : 2.6713433265686035\n","Batch : 130 Loss : 2.6825835704803467\n","Batch : 131 Loss : 2.6898322105407715\n","Batch : 132 Loss : 2.6553776264190674\n","Batch : 133 Loss : 2.643911600112915\n","Batch : 134 Loss : 2.6678497791290283\n","Batch : 135 Loss : 2.6639978885650635\n","Batch : 136 Loss : 2.6582682132720947\n","Batch : 137 Loss : 2.6357648372650146\n","Batch : 138 Loss : 2.666133165359497\n","Batch : 139 Loss : 2.6514103412628174\n","Batch : 140 Loss : 2.616313934326172\n","Batch : 141 Loss : 2.6505236625671387\n","Batch : 142 Loss : 2.6542885303497314\n","Batch : 143 Loss : 2.6544835567474365\n","Batch : 144 Loss : 2.6595072746276855\n","Batch : 145 Loss : 2.6499838829040527\n","Batch : 146 Loss : 2.6510069370269775\n","Batch : 147 Loss : 2.6497538089752197\n","Batch : 148 Loss : 2.640904664993286\n","Batch : 149 Loss : 2.6374597549438477\n","Batch : 150 Loss : 2.6408894062042236\n","Batch : 151 Loss : 2.6214802265167236\n","Batch : 152 Loss : 2.6444859504699707\n","Batch : 153 Loss : 2.624619960784912\n","Batch : 154 Loss : 2.628441095352173\n","Batch : 155 Loss : 2.6340603828430176\n","Batch : 156 Loss : 2.66239857673645\n","Batch : 157 Loss : 2.6462578773498535\n","Batch : 158 Loss : 2.6239798069000244\n","Batch : 159 Loss : 2.6545908451080322\n","Batch : 160 Loss : 2.6264188289642334\n","Batch : 161 Loss : 2.6221485137939453\n","Batch : 162 Loss : 2.617291212081909\n","Batch : 163 Loss : 2.6580004692077637\n","Batch : 164 Loss : 2.6145856380462646\n","Batch : 165 Loss : 2.623565912246704\n","Batch : 166 Loss : 2.6146421432495117\n","Batch : 167 Loss : 2.6097333431243896\n","Batch : 168 Loss : 2.596205711364746\n","Batch : 169 Loss : 2.6112849712371826\n","Batch : 170 Loss : 2.595180034637451\n","Batch : 171 Loss : 2.588778257369995\n","Batch : 172 Loss : 2.6135146617889404\n","Batch : 173 Loss : 2.578981399536133\n","Batch : 174 Loss : 2.576413631439209\n","Batch : 175 Loss : 2.606675386428833\n","Batch : 176 Loss : 2.594905138015747\n","Batch : 177 Loss : 2.5849156379699707\n","Batch : 178 Loss : 2.594475746154785\n","Batch : 179 Loss : 2.6020238399505615\n","Batch : 180 Loss : 2.5788321495056152\n","Batch : 181 Loss : 2.63045334815979\n","Batch : 182 Loss : 2.621809482574463\n","Batch : 183 Loss : 2.6009817123413086\n","Batch : 184 Loss : 2.618260622024536\n","Batch : 185 Loss : 2.596168279647827\n","Batch : 186 Loss : 2.6030378341674805\n","Batch : 187 Loss : 2.587074041366577\n","Batch : 188 Loss : 2.6053361892700195\n","Batch : 189 Loss : 2.6159727573394775\n","Batch : 190 Loss : 2.5711185932159424\n","Batch : 191 Loss : 2.585589647293091\n","Batch : 192 Loss : 2.588693618774414\n","Batch : 193 Loss : 2.60459566116333\n","Batch : 194 Loss : 2.612987518310547\n","Batch : 195 Loss : 2.5823755264282227\n","Batch : 196 Loss : 2.594972848892212\n","Batch : 197 Loss : 2.573711395263672\n","Batch : 198 Loss : 2.585712194442749\n","Batch : 199 Loss : 2.6099119186401367\n","Batch : 200 Loss : 2.5744287967681885\n","Batch : 201 Loss : 2.5663864612579346\n","Batch : 202 Loss : 2.5683276653289795\n","Batch : 203 Loss : 2.576869487762451\n","Batch : 204 Loss : 2.560410261154175\n","Batch : 205 Loss : 2.5900793075561523\n","Batch : 206 Loss : 2.591010808944702\n","Batch : 207 Loss : 2.5847530364990234\n","Batch : 208 Loss : 2.550715208053589\n","Batch : 209 Loss : 2.5665910243988037\n","Batch : 210 Loss : 2.568857192993164\n","Batch : 211 Loss : 2.547334909439087\n","Batch : 212 Loss : 2.5688703060150146\n","Batch : 213 Loss : 2.565187931060791\n","Batch : 214 Loss : 2.5550553798675537\n","Batch : 215 Loss : 2.5859477519989014\n","Batch : 216 Loss : 2.570321559906006\n","Batch : 217 Loss : 2.568525791168213\n","Batch : 218 Loss : 2.56449031829834\n","Batch : 219 Loss : 2.561673164367676\n","Batch : 220 Loss : 2.551039457321167\n","Batch : 221 Loss : 2.549856662750244\n","Batch : 222 Loss : 2.56196928024292\n","Batch : 223 Loss : 2.5470149517059326\n","Batch : 224 Loss : 2.561006546020508\n","Batch : 225 Loss : 2.5533173084259033\n","Batch : 226 Loss : 2.5716798305511475\n","Batch : 227 Loss : 2.558530807495117\n","Batch : 228 Loss : 2.5550217628479004\n","Batch : 229 Loss : 2.5675618648529053\n","Batch : 230 Loss : 2.559922933578491\n","Batch : 231 Loss : 2.5646462440490723\n","Batch : 232 Loss : 2.5406272411346436\n","Batch : 233 Loss : 2.569868564605713\n","Batch : 234 Loss : 2.550607204437256\n","Batch : 235 Loss : 2.5542917251586914\n","Batch : 236 Loss : 2.576482057571411\n","Batch : 237 Loss : 2.547301769256592\n","Batch : 238 Loss : 2.5424411296844482\n","Batch : 239 Loss : 2.569725275039673\n","Batch : 240 Loss : 2.5332579612731934\n","Batch : 241 Loss : 2.5353102684020996\n","Batch : 242 Loss : 2.5441091060638428\n","Batch : 243 Loss : 2.560328960418701\n","Batch : 244 Loss : 2.5641934871673584\n","Batch : 245 Loss : 2.5370914936065674\n","Batch : 246 Loss : 2.5571818351745605\n","Batch : 247 Loss : 2.546199083328247\n","Batch : 248 Loss : 2.5464119911193848\n","Batch : 249 Loss : 2.554187774658203\n","Batch : 250 Loss : 2.58699893951416\n","Batch : 251 Loss : 2.5426764488220215\n","Batch : 252 Loss : 2.590114116668701\n","Batch : 253 Loss : 2.5794875621795654\n","Batch : 254 Loss : 2.561136484146118\n","Batch : 255 Loss : 2.5528934001922607\n","Batch : 256 Loss : 2.548156976699829\n","Batch : 257 Loss : 2.5456602573394775\n","Batch : 258 Loss : 2.5690841674804688\n","Batch : 259 Loss : 2.548146963119507\n","Batch : 260 Loss : 2.551062822341919\n","Batch : 261 Loss : 2.541229009628296\n","Batch : 262 Loss : 2.5407919883728027\n","Batch : 263 Loss : 2.5588998794555664\n","Batch : 264 Loss : 2.5316338539123535\n","Batch : 265 Loss : 2.525170087814331\n","Batch : 266 Loss : 2.554387331008911\n","Batch : 267 Loss : 2.541224241256714\n","Batch : 268 Loss : 2.554339647293091\n","Batch : 269 Loss : 2.562962293624878\n","Batch : 270 Loss : 2.546837091445923\n","Batch : 271 Loss : 2.5340421199798584\n","Batch : 272 Loss : 2.5237886905670166\n","Batch : 273 Loss : 2.5442843437194824\n","Batch : 274 Loss : 2.5326988697052\n","Batch : 275 Loss : 2.5424578189849854\n","Batch : 276 Loss : 2.537419080734253\n","Batch : 277 Loss : 2.5506808757781982\n","Batch : 278 Loss : 2.5619537830352783\n","Epoch : 1 Average Loss : 2.761780023574829\n","Batch : 1 Loss : 2.5420351028442383\n","Batch : 2 Loss : 2.5559520721435547\n","Batch : 3 Loss : 2.538623809814453\n","Batch : 4 Loss : 2.551029682159424\n","Batch : 5 Loss : 2.5418314933776855\n","Batch : 6 Loss : 2.5318496227264404\n","Batch : 7 Loss : 2.521446704864502\n","Batch : 8 Loss : 2.528644561767578\n","Batch : 9 Loss : 2.53302264213562\n","Batch : 10 Loss : 2.530961275100708\n","Batch : 11 Loss : 2.5308310985565186\n","Batch : 12 Loss : 2.544226884841919\n","Batch : 13 Loss : 2.5363545417785645\n","Batch : 14 Loss : 2.549800395965576\n","Batch : 15 Loss : 2.5411949157714844\n","Batch : 16 Loss : 2.5264623165130615\n","Batch : 17 Loss : 2.5574638843536377\n","Batch : 18 Loss : 2.5388736724853516\n","Batch : 19 Loss : 2.550097942352295\n","Batch : 20 Loss : 2.5149409770965576\n","Batch : 21 Loss : 2.5202057361602783\n","Batch : 22 Loss : 2.525613307952881\n","Batch : 23 Loss : 2.5135416984558105\n","Batch : 24 Loss : 2.5052781105041504\n","Batch : 25 Loss : 2.503072500228882\n","Batch : 26 Loss : 2.5089821815490723\n","Batch : 27 Loss : 2.5217604637145996\n","Batch : 28 Loss : 2.545804500579834\n","Batch : 29 Loss : 2.527881145477295\n","Batch : 30 Loss : 2.5345189571380615\n","Batch : 31 Loss : 2.5567572116851807\n","Batch : 32 Loss : 2.515845775604248\n","Batch : 33 Loss : 2.521096706390381\n","Batch : 34 Loss : 2.5446937084198\n","Batch : 35 Loss : 2.546236038208008\n","Batch : 36 Loss : 2.5436155796051025\n","Batch : 37 Loss : 2.5259952545166016\n","Batch : 38 Loss : 2.5307092666625977\n","Batch : 39 Loss : 2.5255465507507324\n","Batch : 40 Loss : 2.5252504348754883\n","Batch : 41 Loss : 2.5226359367370605\n","Batch : 42 Loss : 2.5458106994628906\n","Batch : 43 Loss : 2.5312163829803467\n","Batch : 44 Loss : 2.524200677871704\n","Batch : 45 Loss : 2.51615834236145\n","Batch : 46 Loss : 2.5264289379119873\n","Batch : 47 Loss : 2.5181281566619873\n","Batch : 48 Loss : 2.5354361534118652\n","Batch : 49 Loss : 2.536532163619995\n","Batch : 50 Loss : 2.513108015060425\n","Batch : 51 Loss : 2.511923313140869\n","Batch : 52 Loss : 2.5192832946777344\n","Batch : 53 Loss : 2.510989189147949\n","Batch : 54 Loss : 2.5092554092407227\n","Batch : 55 Loss : 2.5199337005615234\n","Batch : 56 Loss : 2.4968111515045166\n","Batch : 57 Loss : 2.529111862182617\n","Batch : 58 Loss : 2.51041841506958\n","Batch : 59 Loss : 2.522588014602661\n","Batch : 60 Loss : 2.512687921524048\n","Batch : 61 Loss : 2.5185327529907227\n","Batch : 62 Loss : 2.5485949516296387\n","Batch : 63 Loss : 2.5195116996765137\n","Batch : 64 Loss : 2.5386130809783936\n","Batch : 65 Loss : 2.543663740158081\n","Batch : 66 Loss : 2.5378494262695312\n","Batch : 67 Loss : 2.5345823764801025\n","Batch : 68 Loss : 2.5101919174194336\n","Batch : 69 Loss : 2.503714084625244\n","Batch : 70 Loss : 2.5207722187042236\n","Batch : 71 Loss : 2.526355028152466\n","Batch : 72 Loss : 2.5004873275756836\n","Batch : 73 Loss : 2.535871982574463\n","Batch : 74 Loss : 2.5234334468841553\n","Batch : 75 Loss : 2.505918025970459\n","Batch : 76 Loss : 2.5087473392486572\n","Batch : 77 Loss : 2.5123867988586426\n","Batch : 78 Loss : 2.5215470790863037\n","Batch : 79 Loss : 2.5320327281951904\n","Batch : 80 Loss : 2.5275254249572754\n","Batch : 81 Loss : 2.55987286567688\n","Batch : 82 Loss : 2.514268398284912\n","Batch : 83 Loss : 2.518047332763672\n","Batch : 84 Loss : 2.51987361907959\n","Batch : 85 Loss : 2.5187292098999023\n","Batch : 86 Loss : 2.5191216468811035\n","Batch : 87 Loss : 2.5211868286132812\n","Batch : 88 Loss : 2.5309321880340576\n","Batch : 89 Loss : 2.54415225982666\n","Batch : 90 Loss : 2.520761489868164\n","Batch : 91 Loss : 2.5107369422912598\n","Batch : 92 Loss : 2.489384651184082\n","Batch : 93 Loss : 2.516578197479248\n","Batch : 94 Loss : 2.5264530181884766\n","Batch : 95 Loss : 2.5015835762023926\n","Batch : 96 Loss : 2.5271124839782715\n","Batch : 97 Loss : 2.532104969024658\n","Batch : 98 Loss : 2.5104727745056152\n","Batch : 99 Loss : 2.51287579536438\n","Batch : 100 Loss : 2.5206563472747803\n","Batch : 101 Loss : 2.543769359588623\n","Batch : 102 Loss : 2.495621919631958\n","Batch : 103 Loss : 2.50211763381958\n","Batch : 104 Loss : 2.497842788696289\n","Batch : 105 Loss : 2.511716842651367\n","Batch : 106 Loss : 2.5050554275512695\n","Batch : 107 Loss : 2.5091044902801514\n","Batch : 108 Loss : 2.515092372894287\n","Batch : 109 Loss : 2.518019199371338\n","Batch : 110 Loss : 2.5067224502563477\n","Batch : 111 Loss : 2.5144529342651367\n","Batch : 112 Loss : 2.5026328563690186\n","Batch : 113 Loss : 2.495750665664673\n","Batch : 114 Loss : 2.509925603866577\n","Batch : 115 Loss : 2.5079901218414307\n","Batch : 116 Loss : 2.499516248703003\n","Batch : 117 Loss : 2.5268795490264893\n","Batch : 118 Loss : 2.51316237449646\n","Batch : 119 Loss : 2.5111887454986572\n","Batch : 120 Loss : 2.514207601547241\n","Batch : 121 Loss : 2.504138469696045\n","Batch : 122 Loss : 2.4916744232177734\n","Batch : 123 Loss : 2.5166232585906982\n","Batch : 124 Loss : 2.5225532054901123\n","Batch : 125 Loss : 2.549079418182373\n","Batch : 126 Loss : 2.5119738578796387\n","Batch : 127 Loss : 2.5275423526763916\n","Batch : 128 Loss : 2.5038692951202393\n","Batch : 129 Loss : 2.51688289642334\n","Batch : 130 Loss : 2.5189080238342285\n","Batch : 131 Loss : 2.5350892543792725\n","Batch : 132 Loss : 2.5038037300109863\n","Batch : 133 Loss : 2.4964516162872314\n","Batch : 134 Loss : 2.5083231925964355\n","Batch : 135 Loss : 2.5190136432647705\n","Batch : 136 Loss : 2.5082976818084717\n","Batch : 137 Loss : 2.486945629119873\n","Batch : 138 Loss : 2.5184926986694336\n","Batch : 139 Loss : 2.505850076675415\n","Batch : 140 Loss : 2.476243734359741\n","Batch : 141 Loss : 2.509035348892212\n","Batch : 142 Loss : 2.516402244567871\n","Batch : 143 Loss : 2.517256021499634\n","Batch : 144 Loss : 2.5203804969787598\n","Batch : 145 Loss : 2.5081419944763184\n","Batch : 146 Loss : 2.5154004096984863\n","Batch : 147 Loss : 2.5126419067382812\n","Batch : 148 Loss : 2.514824867248535\n","Batch : 149 Loss : 2.5029144287109375\n","Batch : 150 Loss : 2.512354850769043\n","Batch : 151 Loss : 2.489058017730713\n","Batch : 152 Loss : 2.522602081298828\n","Batch : 153 Loss : 2.4931910037994385\n","Batch : 154 Loss : 2.5058274269104004\n","Batch : 155 Loss : 2.511141538619995\n","Batch : 156 Loss : 2.5408802032470703\n","Batch : 157 Loss : 2.5166056156158447\n","Batch : 158 Loss : 2.508826494216919\n","Batch : 159 Loss : 2.5327775478363037\n","Batch : 160 Loss : 2.5073201656341553\n","Batch : 161 Loss : 2.494943618774414\n","Batch : 162 Loss : 2.4958813190460205\n","Batch : 163 Loss : 2.537599563598633\n","Batch : 164 Loss : 2.5015056133270264\n","Batch : 165 Loss : 2.5099940299987793\n","Batch : 166 Loss : 2.50335431098938\n","Batch : 167 Loss : 2.5003340244293213\n","Batch : 168 Loss : 2.498683214187622\n","Batch : 169 Loss : 2.5017471313476562\n","Batch : 170 Loss : 2.4945802688598633\n","Batch : 171 Loss : 2.486494779586792\n","Batch : 172 Loss : 2.5043632984161377\n","Batch : 173 Loss : 2.465362548828125\n","Batch : 174 Loss : 2.4710354804992676\n","Batch : 175 Loss : 2.5063974857330322\n","Batch : 176 Loss : 2.499741554260254\n","Batch : 177 Loss : 2.488766670227051\n","Batch : 178 Loss : 2.496950149536133\n","Batch : 179 Loss : 2.510335683822632\n","Batch : 180 Loss : 2.4896016120910645\n","Batch : 181 Loss : 2.5347461700439453\n","Batch : 182 Loss : 2.527932643890381\n","Batch : 183 Loss : 2.5153634548187256\n","Batch : 184 Loss : 2.5291714668273926\n","Batch : 185 Loss : 2.5098893642425537\n","Batch : 186 Loss : 2.5109786987304688\n","Batch : 187 Loss : 2.502926826477051\n","Batch : 188 Loss : 2.5212230682373047\n","Batch : 189 Loss : 2.5296859741210938\n","Batch : 190 Loss : 2.489661455154419\n","Batch : 191 Loss : 2.5005743503570557\n","Batch : 192 Loss : 2.5088648796081543\n","Batch : 193 Loss : 2.517766237258911\n","Batch : 194 Loss : 2.5310685634613037\n","Batch : 195 Loss : 2.500962734222412\n","Batch : 196 Loss : 2.515122175216675\n","Batch : 197 Loss : 2.4914324283599854\n","Batch : 198 Loss : 2.5104289054870605\n","Batch : 199 Loss : 2.5276646614074707\n","Batch : 200 Loss : 2.495206832885742\n","Batch : 201 Loss : 2.4905383586883545\n","Batch : 202 Loss : 2.491070032119751\n","Batch : 203 Loss : 2.5156455039978027\n","Batch : 204 Loss : 2.4904658794403076\n","Batch : 205 Loss : 2.514958381652832\n","Batch : 206 Loss : 2.522953510284424\n","Batch : 207 Loss : 2.514688014984131\n","Batch : 208 Loss : 2.493474006652832\n","Batch : 209 Loss : 2.4962825775146484\n","Batch : 210 Loss : 2.501774549484253\n","Batch : 211 Loss : 2.4854540824890137\n","Batch : 212 Loss : 2.5062832832336426\n","Batch : 213 Loss : 2.501300096511841\n","Batch : 214 Loss : 2.4869725704193115\n","Batch : 215 Loss : 2.5154285430908203\n","Batch : 216 Loss : 2.5075154304504395\n","Batch : 217 Loss : 2.505190134048462\n","Batch : 218 Loss : 2.5006046295166016\n","Batch : 219 Loss : 2.497101306915283\n","Batch : 220 Loss : 2.493283987045288\n","Batch : 221 Loss : 2.492063045501709\n","Batch : 222 Loss : 2.5048983097076416\n","Batch : 223 Loss : 2.487354278564453\n","Batch : 224 Loss : 2.503851890563965\n","Batch : 225 Loss : 2.4969351291656494\n","Batch : 226 Loss : 2.5121355056762695\n","Batch : 227 Loss : 2.4930312633514404\n","Batch : 228 Loss : 2.497744560241699\n","Batch : 229 Loss : 2.5093698501586914\n","Batch : 230 Loss : 2.510934352874756\n","Batch : 231 Loss : 2.508927345275879\n","Batch : 232 Loss : 2.4885499477386475\n","Batch : 233 Loss : 2.5130321979522705\n","Batch : 234 Loss : 2.4975030422210693\n","Batch : 235 Loss : 2.5031027793884277\n","Batch : 236 Loss : 2.521221160888672\n","Batch : 237 Loss : 2.489997625350952\n","Batch : 238 Loss : 2.4914798736572266\n","Batch : 239 Loss : 2.5137996673583984\n","Batch : 240 Loss : 2.4815640449523926\n","Batch : 241 Loss : 2.486384153366089\n","Batch : 242 Loss : 2.49251389503479\n","Batch : 243 Loss : 2.5109446048736572\n","Batch : 244 Loss : 2.504955291748047\n","Batch : 245 Loss : 2.4849605560302734\n","Batch : 246 Loss : 2.5062570571899414\n","Batch : 247 Loss : 2.493788003921509\n","Batch : 248 Loss : 2.493891477584839\n","Batch : 249 Loss : 2.4965741634368896\n","Batch : 250 Loss : 2.5361075401306152\n","Batch : 251 Loss : 2.4948570728302\n","Batch : 252 Loss : 2.5347225666046143\n","Batch : 253 Loss : 2.5223562717437744\n","Batch : 254 Loss : 2.505096435546875\n","Batch : 255 Loss : 2.502084493637085\n","Batch : 256 Loss : 2.499769687652588\n","Batch : 257 Loss : 2.4948768615722656\n","Batch : 258 Loss : 2.512828826904297\n","Batch : 259 Loss : 2.503325939178467\n","Batch : 260 Loss : 2.500875473022461\n","Batch : 261 Loss : 2.488818645477295\n","Batch : 262 Loss : 2.4981606006622314\n","Batch : 263 Loss : 2.512312650680542\n","Batch : 264 Loss : 2.4884016513824463\n","Batch : 265 Loss : 2.4832677841186523\n","Batch : 266 Loss : 2.5066230297088623\n","Batch : 267 Loss : 2.494877338409424\n","Batch : 268 Loss : 2.5030293464660645\n","Batch : 269 Loss : 2.5164074897766113\n","Batch : 270 Loss : 2.5021250247955322\n","Batch : 271 Loss : 2.488210916519165\n","Batch : 272 Loss : 2.4842097759246826\n","Batch : 273 Loss : 2.498802423477173\n","Batch : 274 Loss : 2.4907243251800537\n","Batch : 275 Loss : 2.5083541870117188\n","Batch : 276 Loss : 2.4905338287353516\n","Batch : 277 Loss : 2.5068812370300293\n","Batch : 278 Loss : 2.5174808502197266\n","Epoch : 2 Average Loss : 2.513019561767578\n"],"name":"stdout"}]},{"metadata":{"id":"ys-h2Hkvhy2s","colab_type":"code","outputId":"df8396fa-6066-40ef-cb8f-8edfd21c592b","executionInfo":{"status":"ok","timestamp":1544576147049,"user_tz":300,"elapsed":227,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"cell_type":"code","source":["# Just to check if all data was loaded into GPU\n","print(next(model.parameters()).is_cuda)\n","print(feature.is_cuda)\n","print(label.is_cuda) # Labels dont need to be loaded"],"execution_count":0,"outputs":[{"output_type":"stream","text":["True\n","True\n","False\n"],"name":"stdout"}]},{"metadata":{"id":"yVB0hYsu-6do","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","\n","# CPU based model and training, too slow Colab timing out\n","# Creating the model and run it on CPU\n","\n","model_cpu = LSTM(sequence_length=128, \n","             input_size=len(character_to_integer), \n","             hidden_size=512, \n","             batch_size=128)\n","\n","# Using Adam as Optimizer and CrossEntropyLoss as the loss function\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model_cpu.parameters(), lr = 0.01)\n","\n","# Training loop\n","\n","for epoch in range(10):\n","  \n","  # Initialize hidden and cell state to zero at the beginning\n","  hidden_and_cell_state = model_cpu.initialize_hidden_and_cell_states()\n","   \n","  # Creating instance of dataloader class\n","  T = TextDataLoader(training_set, 128,128)\n","\n","  for i, (feature, label) in enumerate(T.make_batches()):\n","    \n","    # One-hot encode inputs, convert into tensor and transpose them\n","    feature = torch.from_numpy(to_categorical(feature, num_classes=model_cpu.input_size).transpose([1, 0, 2]))\n","    # Converting input to FloatTensor\n","    feature = feature.float()\n","    \n","    # Transpose labels and convert into LongTensor\n","    label = torch.from_numpy(label.T).type(torch.LongTensor)\n","      \n","    optimizer.zero_grad() # Makes sure the gradients are initially zero\n","    \n","    out = model_cpu(feature, hidden_and_cell_state) # Forward pass\n","    \n","    loss = criterion(out,label.contiguous().view(128*128)) # Computing loss\n","    \n","    loss.backward() # Back-Prop the loss / Backward Pass\n","\n","    optimizer.step() # Update the gradients\n","    \n","    print(f'Batch : {i+1} Loss : {loss}')\n","\n","  print(f'Epoch : {epoch+1} Loss : {loss}')\n","\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i2_jVizDm491","colab_type":"text"},"cell_type":"markdown","source":["### **Saving model data to google drive and fetching it for future use**"]},{"metadata":{"id":"aYIilCE5l3AY","colab_type":"code","outputId":"6e4d5311-6b84-4279-f4c2-a78fb9c12775","executionInfo":{"status":"ok","timestamp":1543028584377,"user_tz":300,"elapsed":3007,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["# Saving the model\n","torch.save(model.state_dict(), 'model_data_initial_state_100.pt')\n","\n","# Check size of model state dictionary\n","!ls -l"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 41700\n","-rw-r--r-- 1 root root     2578 Nov 23 18:02 adc.json\n","-rw-r--r-- 1 root root 39744985 Nov 24 03:03 model_data_initial_state_100.pt\n","drwxr-xr-x 2 root root     4096 Nov 20 18:17 sample_data\n","-rw-r--r-- 1 root root  2943701 Nov 23 18:02 text_corpus\n"],"name":"stdout"}]},{"metadata":{"id":"fFN5_3yBmXMb","colab_type":"code","colab":{}},"cell_type":"code","source":["# Upload to drive and work with it\n","from google.colab import files\n","files.download('model_data_initial_state_100.pt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8-OUd1Q5mf-y","colab_type":"code","colab":{}},"cell_type":"code","source":["# Fetching the model_data file from google drive\n","fileId = drive.CreateFile({'id': ''}) # id of model_data file\n","file = fileId['title']\n","fileId.GetContentFile(file)  # Save Drive file as a local file"],"execution_count":0,"outputs":[]},{"metadata":{"id":"niBN1wEcm2pK","colab_type":"code","colab":{}},"cell_type":"code","source":["# Creating a model and Loading the state data saved earlier\n","\n","# model = LSTM(sequence_length=128, \n","#             input_size=len(character_to_integer), \n","#             hidden_size=512, \n","#             batch_size=128).to(device) # If running on GPU\n","\n","model = LSTM(sequence_length=128, \n","             input_size=len(character_to_integer), \n","             hidden_size=512, \n","             batch_size=128) # if running on CPU\n","\n","model.load_state_dict(torch.load('model_data.pt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dfx0urlntZh1","colab_type":"text"},"cell_type":"markdown","source":["### **Validating the model on the validation data set**"]},{"metadata":{"id":"ABkElr1m_KNB","colab_type":"code","outputId":"f4085f57-13f9-4b0f-8dc1-949e4795865e","executionInfo":{"status":"ok","timestamp":1544686735429,"user_tz":300,"elapsed":273902,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":11777}},"cell_type":"code","source":["# Validation loop\n","model.eval()\n","\n","for epoch in range(10):\n","  \n","  # Initialize hidden and cell state to zero at the beginning\n","  hidden_and_cell_state = model.initialize_hidden_and_cell_states()\n","   \n","  # Creating instance of dataloader class\n","  T = TextDataLoader(validation_set, 128,128)\n","\n","  for i, (feature, label) in enumerate(T.make_batches()):\n","    \n","    # One-hot encode inputs, convert into tensor and transpose them\n","    feature = torch.from_numpy(to_categorical(feature, num_classes=model.input_size).transpose([1, 0, 2]))\n","    # Converting input to FloatTensor\n","    feature = feature.float()\n","    feature.requires_grad_()\n","    feature = feature.to(device)\n","    \n","    # Transpose labels and convert into LongTensor\n","    label = torch.from_numpy(label.T).type(torch.LongTensor)\n","       \n","    out = model(feature, hidden_and_cell_state) # Forward pass\n","    \n","    loss = criterion(out,label.contiguous().view(128*128)) # Computing loss\n","    \n","    print(f'Batch : {i+1} Loss : {loss}')\n","\n","  print(f'Epoch : {epoch+1} Loss : {loss}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 1 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 2 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 3 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 4 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 5 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 6 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 7 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 8 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 9 Loss : 2.527137517929077\n","Batch : 1 Loss : 2.547416925430298\n","Batch : 2 Loss : 2.532693862915039\n","Batch : 3 Loss : 2.529435634613037\n","Batch : 4 Loss : 2.5287697315216064\n","Batch : 5 Loss : 2.5254032611846924\n","Batch : 6 Loss : 2.5405335426330566\n","Batch : 7 Loss : 2.530756711959839\n","Batch : 8 Loss : 2.512718439102173\n","Batch : 9 Loss : 2.5271551609039307\n","Batch : 10 Loss : 2.520352602005005\n","Batch : 11 Loss : 2.532042980194092\n","Batch : 12 Loss : 2.5148468017578125\n","Batch : 13 Loss : 2.519270658493042\n","Batch : 14 Loss : 2.5226192474365234\n","Batch : 15 Loss : 2.532123565673828\n","Batch : 16 Loss : 2.516371965408325\n","Batch : 17 Loss : 2.5177488327026367\n","Batch : 18 Loss : 2.5201001167297363\n","Batch : 19 Loss : 2.5183284282684326\n","Batch : 20 Loss : 2.5283570289611816\n","Batch : 21 Loss : 2.546668291091919\n","Batch : 22 Loss : 2.5312142372131348\n","Batch : 23 Loss : 2.530093193054199\n","Batch : 24 Loss : 2.5212182998657227\n","Batch : 25 Loss : 2.497713565826416\n","Batch : 26 Loss : 2.524336814880371\n","Batch : 27 Loss : 2.5107977390289307\n","Batch : 28 Loss : 2.4991650581359863\n","Batch : 29 Loss : 2.514199733734131\n","Batch : 30 Loss : 2.510359287261963\n","Batch : 31 Loss : 2.5287680625915527\n","Batch : 32 Loss : 2.5158584117889404\n","Batch : 33 Loss : 2.524876832962036\n","Batch : 34 Loss : 2.535763740539551\n","Batch : 35 Loss : 2.5168230533599854\n","Batch : 36 Loss : 2.4965131282806396\n","Batch : 37 Loss : 2.5082075595855713\n","Batch : 38 Loss : 2.514420509338379\n","Batch : 39 Loss : 2.5092902183532715\n","Batch : 40 Loss : 2.5167646408081055\n","Batch : 41 Loss : 2.5025482177734375\n","Batch : 42 Loss : 2.504880428314209\n","Batch : 43 Loss : 2.5181660652160645\n","Batch : 44 Loss : 2.524807929992676\n","Batch : 45 Loss : 2.529182195663452\n","Batch : 46 Loss : 2.510552167892456\n","Batch : 47 Loss : 2.49273943901062\n","Batch : 48 Loss : 2.5327529907226562\n","Batch : 49 Loss : 2.5227034091949463\n","Batch : 50 Loss : 2.527479648590088\n","Batch : 51 Loss : 2.522674322128296\n","Batch : 52 Loss : 2.5148844718933105\n","Batch : 53 Loss : 2.514347791671753\n","Batch : 54 Loss : 2.516763925552368\n","Batch : 55 Loss : 2.532748222351074\n","Batch : 56 Loss : 2.5446770191192627\n","Batch : 57 Loss : 2.532825469970703\n","Batch : 58 Loss : 2.5513744354248047\n","Batch : 59 Loss : 2.541637659072876\n","Batch : 60 Loss : 2.541895866394043\n","Batch : 61 Loss : 2.5557165145874023\n","Batch : 62 Loss : 2.5427958965301514\n","Batch : 63 Loss : 2.5137593746185303\n","Batch : 64 Loss : 2.5435826778411865\n","Batch : 65 Loss : 2.524822473526001\n","Batch : 66 Loss : 2.533789873123169\n","Batch : 67 Loss : 2.50972843170166\n","Batch : 68 Loss : 2.5106184482574463\n","Batch : 69 Loss : 2.527137517929077\n","Epoch : 10 Loss : 2.527137517929077\n"],"name":"stdout"}]},{"metadata":{"id":"NQvoBlnewNwB","colab_type":"text"},"cell_type":"markdown","source":["Validation loop achieves loss values similar to that of a converged network "]},{"metadata":{"id":"Ku-8tEefKSTm","colab_type":"text"},"cell_type":"markdown","source":["### **Predict output**"]},{"metadata":{"id":"ghTCynGWKWQW","colab_type":"code","colab":{}},"cell_type":"code","source":["def predict_next_char(model, input_char, sequence_length = 128):\n","  \n","  model.eval # set model to eval mode\n","  \n","  # Set initail hidden and cell state to 0, but now batch size is 1\n","  initial_hidden_and_cell_state = (torch.zeros(1, model.hidden_size, device = device),\n","                                      torch.zeros(1, model.hidden_size, device = device))\n","  \n","  \n","  # placeholder for the generated text\n","  seq = np.empty(sequence_length+1)\n","  seq[0] = character_to_integer[input_char]\n","  \n","  # Encode the input character, (1, input_size)\n","  input_char = torch.from_numpy(to_categorical(character_to_integer[input_char], \n","                                               num_classes=model.input_size))\n","  input_char = input_char.float()\n","  \n","  input_char.requires_grad_()\n","  input_char = input_char.to(device)\n","  \n","  # Add fake dimension, as input to network needs to be batches\n","  input_char = input_char.unsqueeze(0)\n","  \n","  # Forward pass, without dropout layers \n","  for t in range(sequence_length):\n","    out = model.lstm1(input_char, initial_hidden_and_cell_state)\n","    hidden1, cell1 = out\n","    \n","    out = model.lstm2(hidden1, initial_hidden_and_cell_state)\n","    hidden2, cell2 = out\n","\n","\n","    out = model.lstm3(hidden2, initial_hidden_and_cell_state)\n","    hidden3, cell3 = out\n","\n","    out = model.lstm4(hidden3, initial_hidden_and_cell_state)\n","    hidden4, cell4 = out\n","\n","    # Passing output of LSTMCells through fc layers\n","    out = model.fc1(hidden4)\n","\n","    out = model.fc2(out)\n","\n","    # Applying softmax, to get probabilities\n","    out = F.softmax(out, dim=1)\n","    \n","    # Out is now a vector of (1, input_size)\n","    # Get top 5 best predictions based on softmax probabilities\n","    prob,top_characters = out.topk(5)\n","    \n","    top_characters =top_characters.to(\"cpu\")\n","    prob =prob.to(\"cpu\")\n","                                      \n","    top_characters = top_characters.squeeze().numpy()\n","    prob = prob.detach().squeeze().numpy()\n","\n","\n","    \n","    char = np.random.choice(top_characters, p = prob/prob.sum())\n","\n","    # append the character to the output sequence\n","    seq[t+1] = char\n","\n","    # prepare the character to be fed to the next LSTM cell\n","    char = to_categorical(char, num_classes=model.input_size)\n","    char = torch.from_numpy(char).unsqueeze(0)\n","    \n","    # Type conversion before feeding the new char into LSTM\n","    #input_char = char.byte()\n","    input_char = char.float()\n","    #input_char.requires_grad_()\n","    input_char = input_char.to(device)\n","    \n","  return seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ag59To4FQhn8","colab_type":"code","outputId":"302e0e39-1aa7-497c-fd31-ffa607683249","executionInfo":{"status":"ok","timestamp":1544703306927,"user_tz":300,"elapsed":1313,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"cell_type":"code","source":["# Testing on smaller sequence length\n","seq = predict_next_char(model, 'A', sequence_length = 128)\n","\n","# Convert sequence elements type to int, \n","# else we wont be able to use them as keys for integer_to_character dictionary \n","seq = seq.astype(int)\n","print(seq)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["[72 40 11 11 19 11 71 71 40 10 71 40 71 64 19 81 64 81 40 11 11 11 40 11\n"," 11 40 11 19 40 71 40 10 71 40 19 40 19 19 19 40 10 19 71 40 40 10 71 19\n"," 19 40 19 19 19 40 40 19 40 71 40 11 71 40 11 71 11 81 11 71 40 19 40 19\n"," 71 19 19 71 11 40 10 40 11 11 19 71 40 19 40 11 81 64 81 11 71 71 64 40\n"," 10 81 40 19 71 11 40 10 11 81 19 40 19 71 19 40 40 71 19 81 19 81 64 64\n"," 81 40 19 40 71 64 71 19 11]\n"],"name":"stdout"}]},{"metadata":{"id":"w1NFo9KptYR7","colab_type":"code","outputId":"b786c525-66e8-4f6d-c645-29400b2439f4","executionInfo":{"status":"ok","timestamp":1544703308647,"user_tz":300,"elapsed":353,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["# Converting output from encoded numbers to letters to see what the model \n","# actually predicted.\n","character_sequence = [integer_to_character[i] for i in seq]\n","print(character_sequence)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["['A', 't', ' ', ' ', 'e', ' ', 's', 's', 't', 'h', 's', 't', 's', 'a', 'e', 'n', 'a', 'n', 't', ' ', ' ', ' ', 't', ' ', ' ', 't', ' ', 'e', 't', 's', 't', 'h', 's', 't', 'e', 't', 'e', 'e', 'e', 't', 'h', 'e', 's', 't', 't', 'h', 's', 'e', 'e', 't', 'e', 'e', 'e', 't', 't', 'e', 't', 's', 't', ' ', 's', 't', ' ', 's', ' ', 'n', ' ', 's', 't', 'e', 't', 'e', 's', 'e', 'e', 's', ' ', 't', 'h', 't', ' ', ' ', 'e', 's', 't', 'e', 't', ' ', 'n', 'a', 'n', ' ', 's', 's', 'a', 't', 'h', 'n', 't', 'e', 's', ' ', 't', 'h', ' ', 'n', 'e', 't', 'e', 's', 'e', 't', 't', 's', 'e', 'n', 'e', 'n', 'a', 'a', 'n', 't', 'e', 't', 's', 'a', 's', 'e', ' ']\n"],"name":"stdout"}]},{"metadata":{"id":"xp6pyrHI0q8s","colab_type":"code","outputId":"39feb7c5-1ace-4372-ec43-077ccfacd92d","executionInfo":{"status":"ok","timestamp":1544703313395,"user_tz":300,"elapsed":534,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Single round test\n","res = ''.join(character_sequence)\n","print(res)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["At  e ssthstsaenant   t  t etsthsteteeethestthseeteeettetst st s n stetesees tht  estet nan ssathntes th netesettsenenaantetsase \n"],"name":"stdout"}]},{"metadata":{"id":"La1gzMBcxN8b","colab_type":"text"},"cell_type":"markdown","source":["### **Make the model write stuff from a seed character**"]},{"metadata":{"id":"7m0GHqmJxNBq","colab_type":"code","outputId":"504fb8ec-2334-42c0-dc19-f7c72a7a2356","executionInfo":{"status":"ok","timestamp":1544688653095,"user_tz":300,"elapsed":1762885,"user":{"displayName":"Soumya Mohanty","photoUrl":"https://lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s64/photo.jpg","userId":"01526258923113137091"}},"colab":{"base_uri":"https://localhost:8080/","height":205}},"cell_type":"code","source":["# Run model on validation data and see what model writes\n","# Starting seed is a\n","model.eval()\n","\n","# empty list to store model output\n","model_output_list = []\n","\n","for epoch in range(10):\n","  \n","  # Initialize hidden and cell state to zero at the beginning\n","  hidden_and_cell_state = model.initialize_hidden_and_cell_states()\n","   \n","  # Creating instance of dataloader class\n","  T = TextDataLoader(validation_set, 128,128)\n","\n","  for i, (feature, label) in enumerate(T.make_batches()):\n","    \n","    # One-hot encode inputs, convert into tensor and transpose them\n","    feature = torch.from_numpy(to_categorical(feature, num_classes=model.input_size).transpose([1, 0, 2]))\n","    # Converting input to FloatTensor\n","    feature = feature.float()\n","    feature.requires_grad_()\n","    feature = feature.to(device)\n","    \n","    # Transpose labels and convert into LongTensor\n","    label = torch.from_numpy(label.T).type(torch.LongTensor)\n","       \n","    out = model(feature, hidden_and_cell_state) # Forward pass\n","    \n","    loss = criterion(out,label.contiguous().view(128*128)) # Computing loss\n","    \n","    model_output_list.append(''.join([integer_to_character[k] for k in predict_next_char(model, 'A', sequence_length = 1024)]))\n","\n","  print(f'Model output for epoch {epoch+1} :')\n","  print(model_output_list[epoch])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model output for epoch 1 :\n","Atht tthn  nsest enateetenstseteten naetsen    n nsen  t nttes t nanattthse tsentteesttesanttheestst  ees esetet tentesatheenststsenat tsttsaenat  eet  t nathss ths sss sstht tenasenans ene ttsttsatt  t te n tttet tt ssentttsttttttts t eetse thnsaathstee e  eseteetents settethssatth ssthethnettessth tt naette nes nt stet  naeensethses  nee en  nteeeeeeee  ethentetesats s t natetsat th tt ete  thee saees senatett san  eesethtt esstht  n ttetenssaesesstettestthsantttttt ss t ene e setesenenen   t stthte sananatt et  stttsenaessthethe nas tsaat sathtee n  en  t ntsseestths s enassaesaenatesaaeeesen tht ne th ettent   ees te ns sae th esth en ethts  nteettttetenaet ttsststhnt  sat  nssaaanstht  ts  n t tesanesat stsen t e enesaese ens tettt nenessatt ns  eensstt sas tsent th  thetsaaeneethtenth nth tst t s sae testte etsenttt nsstheet  ee esae stst seths ts eenthen sthetese eten  tttssee nenee etssat n s ssathnt tt sss nteset eee  tt ttttetessttsetthth st  thtstsse st tsttssastte tenttentttessts sts   ns ntsthsste\n","Model output for epoch 2 :\n","Asss  th t  tt esthenaenathsatet tettsentst ettsensant tt s  ths   ts ssete tsat t etettssteeeest   ntst eeseneen   s naen n   the ten tsthnt enesest tthtt t seen enestttht t es stenttsae n  thts nseteene sensan en stesstthee   nte t  thenaeneeen eenestethsatenttsantetttesthnsstts nenesanth eeeseseth setheeets  e tte thn n  th  s eesat ee   e n   ettthneents ss essts    ntt st n  essst nst t n en nes nsathnsaess  ensattte tse st thsetheneetessthtteee  th ttht nt     n n tsaatstettseen tth   tst sth thttetts  esth  tettt tet ttentthes esestth nesaanss es et tetts nthes t t tttsethentst tst   th et   ente  tsthntseen e te e tet es tt esaet  es tsenetsat stttthen  neenttett ttsaassaaeess enethne ses nte te sathest  tte t ttsst esethsthte tsthensseteese eeeeetet  sthtttsete e satenaasenethes  t  stenstsaaetttst t est se ns st tens eenseeeete t  t  essss sesst e tteseenaeente st enthett thtt ntheeth naense n nae teentt nttststtheeethee ssst stttsent th  ststhtt sanaste t  neethte enttetteessansente  et n ts  eene e\n","Model output for epoch 3 :\n","Antentteeneneenthne tetsetsteeten  et t  esstteseetsan nesstsseetsthttthstsat sseenattssaeset etsttsaents eenests tt  eees  neethtsattts ttsthestt  sees  t saenan ssetth ten  etthetsesaetesenssaseee nathetensset  nasansstet saseettentss nethntesaaeths  teth   tes saanteeneessaeteen se ss  thneet ssstt es e  t sst  t t ththntth eeetttttts s nst nte thtenen ee nss tss  t tssthte ssanaet tsethessen ttt st  sseesttenstsen n  essetesatt eees tsssenaentst stsasset naeent n nsenasetttthssteteeetsenesaantentsasateents nethsatttesen setsstesae nst n n sassenes ttst nethtstht essseentes eeseesethetetttesaesaaen nsaaenest  tst t sessenssaeetthsatsthth et tttheetethethn nst thnet   t sseten n  nats n ns t t  se  tete estet nasanasth ees sss ssetssass t  eenaste tht  sstet seseesthsat teteee tt tth   ntssstet  t t ests      esantt etsane  tt net e enee t ntss  net s  nets ssttessseenee  tte nateest enttesssatethsasttt ts  eeenetts se es en t enanetetthtstsath  tst ethnt   tthstenees   thnans  nes  tsste  enaeesthetenetths \n","Model output for epoch 4 :\n","Atstesthnss st ttestsst  st nstteet naeththnt s thtttt n ensatsaetthnathnessss tt nenssatt   en e s t te ne tteee enenttstetteth sasetsas teesttetee    thnatenteenatststeteneeeestttenasatts etttee ts nae ethnst ttt nss thn ssst  th n t et eneeetheeenstettt s ts thntthtsese naeth naaeeentthettetttt ntestsaent s naaes tet s t teneen etthnt ns   etth ntt see etttttsaae steteee tteene  ete t naessthent nttsenesatsat ee t nsstt nthnenethtt nsaes t n ttt tthn  neeene  tttttetth san s netssthssaane e esths sttht   n t eteets s sts entte enenth t s  te  tsanenaest neset ttenethnesth naneete t tesaett tset  thnaanth nt     nansssaaetess  nant tsaet eet s  ttetethsat  ttstsaes tst seesseths tenetthnaaasets sttttth et sttt sattensthntttt  ss en sth tht natsteththnsse  et nethestetets sth  t t ttst  s t  s  tte  thn stheths  ssathset  eest neee t  t eetsaaen ene   etttetet ssthsessts enesat tse    tt enetssssaesstent tsatsenste eneeet  esaessthttht eetts st e   sasenttesaathe  tete n ett s s  ens s nsassthen neenth esttet\n","Model output for epoch 5 :\n","An t t tseetst  en eene nsaneee e tensstthnstensanth  sestheth tthte  tethnsenatststhnths t s s tthnset saeesat saeenaensststsat testessthent tt eets sansssethtete es  sastenen setttsssaaeete t  sstteeeeneteene sanensat   esantteenae etenet th nass     nasettts tt teets t nstt tttenats ts n  en t ttsaaatetsetenanaatttthnaasent   ns ns   t ste t esttss t   estt t saens tenen  sats etse s tt ts t tss nttet   tt sasetthts ntettt th s   ensete  es tssesae ntettee  tths tssthnete ee stensesthsatetenettts sseneeet thtstt th  thsttsst nat enset n  t s tsstentenat t t eststhe enetenatentten nthsttteent thtt tsen  ees t stt     sths eent tesaaantethst s ee tt n  saeeen e sattent t  n e tenet teenas ethttthesaaesest enaaentsethenattstssattt   etesae eessatee t     ensenaensans t ttssstth  t s s t enes  tttthnas n   eeeens t ts nseth  th  tsttstene t nten  thnesanethtsses ten   nest th     thnthstett n ttst nastt sesae nesth ethsseenttte ttsat  enaathste tts t  esans    n ethststhssstenesesatttt     ntt  e ess  ssset ess\n"],"name":"stdout"}]}]}